# ğŸš€ Tech Challenge - Pipeline de Dados para Bovespa

## ğŸ“Œ VisÃ£o Geral
Este projeto consiste na construÃ§Ã£o de um **pipeline de dados completo** para extrair, processar e analisar os dados do pregÃ£o da B3 (**Bovespa**). A soluÃ§Ã£o serÃ¡ implementada tanto na **AWS** quanto no **Google Cloud**, permitindo comparaÃ§Ã£o entre as duas arquiteturas.

## ğŸ¯ Objetivos
- ğŸ•µï¸â€â™‚ï¸ Extrair dados do pregÃ£o da B3 via **scraping**.
- ğŸ’¾ Armazenar os dados brutos em um **data lake**.
- âš™ï¸ Processar e transformar os dados utilizando ferramentas de **ETL**.
- ğŸ“Š Disponibilizar os dados refinados para consulta em **SQL**.
- ğŸ“ˆ Criar visualizaÃ§Ãµes a partir dos dados processados.

## ğŸ› ï¸ Tecnologias Utilizadas
### â˜ï¸ AWS Stack
- **Amazon S3**: Armazenamento de dados brutos e refinados.
- **AWS Lambda**: Trigger para iniciar o processamento dos dados.
- **AWS Glue**: ETL para transformaÃ§Ã£o dos dados.
- **AWS Glue Catalog**: CatÃ¡logo dos metadados.
- **Amazon Athena**: Consulta SQL sobre os dados processados.
- **Amazon QuickSight** (opcional): VisualizaÃ§Ã£o de dados.

### â˜ï¸ Google Cloud Stack
- **Google Cloud Storage (GCS)**: Armazenamento de dados brutos e refinados.
- **Google Cloud Functions**: Trigger para iniciar o processamento dos dados.
- **Google Cloud Dataflow / Dataprep**: ETL para transformaÃ§Ã£o dos dados.
- **Google Cloud Data Catalog**: CatÃ¡logo dos metadados.
- **Google BigQuery**: Consulta SQL sobre os dados processados.
- **Looker / Google Data Studio** (opcional): VisualizaÃ§Ã£o de dados.

## ğŸ”„ Estrutura do Pipeline
1. **ğŸ“¥ Coleta de Dados**: Web scraping dos dados da B3.
2. **ğŸ’¾ Armazenamento**: Salvamento dos dados brutos no formato **Parquet** com partiÃ§Ã£o diÃ¡ria.
3. **âš¡ Trigger do Processamento**: Upload dos dados dispara uma **funÃ§Ã£o serverless** (AWS Lambda / Google Cloud Function).
4. **ğŸ› ï¸ ETL**: 
   - ğŸ”¢ Agrupamento numÃ©rico e sumarizaÃ§Ã£o.
   - âœï¸ RenomeaÃ§Ã£o de colunas.
   - ğŸ“… CÃ¡lculo com campos de data.
5. **ğŸ’¾ Armazenamento dos Dados Processados**: Os dados refinados sÃ£o armazenados no data lake.
6. **ğŸ“‚ CatalogaÃ§Ã£o**: Os dados sÃ£o catalogados automaticamente.
7. **ğŸ” Consulta SQL**: Os dados ficam disponÃ­veis para anÃ¡lise no Athena ou BigQuery.
8. **ğŸ“Š VisualizaÃ§Ã£o** (opcional): ConstruÃ§Ã£o de dashboards interativos.

## ğŸš€ Como Implementar

### 1ï¸âƒ£ ImplementaÃ§Ã£o na AWS
#### â˜ï¸ Configurar o Armazenamento
1. Criar um **bucket S3** para armazenar os dados.
2. Configurar permissÃµes adequadas no bucket.

#### âš¡ Criar a FunÃ§Ã£o Lambda
3. Criar uma **AWS Lambda** que dispara o job no Glue.
4. Configurar o trigger para ser ativado ao upload de arquivos no S3.

#### ğŸ› ï¸ Configurar o Glue
5. Criar um **Job AWS Glue** no modo visual com as transformaÃ§Ãµes necessÃ¡rias.
6. Configurar a saÃ­da dos dados no bucket **refined**.
7. Criar o **Glue Catalog** para catalogar os dados automaticamente.

#### ğŸ” Consulta e VisualizaÃ§Ã£o
8. Configurar o **Athena** para consultas SQL.
9. (Opcional) Criar dashboards no **QuickSight**.

### 2ï¸âƒ£ ImplementaÃ§Ã£o no Google Cloud
#### â˜ï¸ Configurar o Armazenamento
1. Criar um **bucket no Google Cloud Storage (GCS)** para armazenar os dados.
2. Configurar permissÃµes adequadas no bucket.

#### âš¡ Criar a FunÃ§Ã£o Cloud Function
3. Criar uma **Google Cloud Function** para disparar o job no Dataflow.
4. Configurar o trigger para ser ativado ao upload de arquivos no GCS.

#### ğŸ› ï¸ Configurar o Dataflow
5. Criar um **pipeline Dataflow (Apache Beam)** com as transformaÃ§Ãµes necessÃ¡rias.
6. Configurar a saÃ­da dos dados no bucket **refined**.
7. Criar o **Data Catalog** para catalogar os dados automaticamente.

#### ğŸ” Consulta e VisualizaÃ§Ã£o
8. Configurar o **BigQuery** para consultas SQL.
9. (Opcional) Criar dashboards no **Looker / Data Studio**.

## ğŸ“ Estrutura do RepositÃ³rio
```
/
|-- aws/                   # ImplementaÃ§Ã£o AWS
|   |-- lambda/            # CÃ³digo da AWS Lambda
|   |-- glue/              # ConfiguraÃ§Ã£o do Glue Job
|   |-- athena/            # Scripts SQL para consultas
|   |-- quicksight/        # Dashboards (se aplicÃ¡vel)
|
|-- gcp/                   # ImplementaÃ§Ã£o Google Cloud
|   |-- cloud_function/    # CÃ³digo da Cloud Function
|   |-- dataflow/          # Pipeline Dataflow
|   |-- bigquery/          # Scripts SQL para consultas
|   |-- looker/            # Dashboards (se aplicÃ¡vel)
|
|-- data/                  # Exemplo de datasets
|-- notebooks/             # Jupyter Notebooks para anÃ¡lise
|-- README.md              # DocumentaÃ§Ã£o do projeto
```

## ğŸ¤ Como Contribuir
1. FaÃ§a um **fork** do repositÃ³rio.
2. Crie uma **branch** para sua feature (`git checkout -b minha-feature`).
3. FaÃ§a **commit** das suas alteraÃ§Ãµes (`git commit -m 'Adiciona nova feature'`).
4. FaÃ§a um **push** para a branch (`git push origin minha-feature`).
5. Abra um **Pull Request**.

## ğŸ“© Contato
Caso tenha dÃºvidas ou sugestÃµes, entre em contato via [GitHub Issues](https://github.com/andreluizpedroso/Pipeline-de-dados-Bovespa).

---

Este projeto faz parte do **Tech Challenge - Fase 2 - Machine Learning AvanÃ§ado**.

